{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599023675084",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1:分词\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"let's go the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "let's go the park."
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the DET\nhourse NOUN\ngalloped VERB\ndown ADP\nthen ADV\nfield NOUN\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the hourse galloped down then field</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step2:向量化（操作暂忽略）\n",
    "#step3:词性标注\n",
    "for token in doc:\n",
    "    print (token.text,token.pos_)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Europe 0 6 LOC\nShanghai 11 19 GPE\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Europe\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n</mark>\n and \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Shanghai\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step4:语法分析or依存分析（操作暂跳过）\n",
    "# step5:命名实体识别\n",
    "doc=nlp(u'Europe and Shanghai')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_)\n",
    "    #Europe属于地点,上海属于城市  \n",
    "from spacy import displacy#可视化\n",
    "displacy.render(doc,style='ent')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#规则匹配：https://www.jianshu.com/p/488e29470755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'whose', 'here', 'make', 'all', 'he', 'his', 'thru', 'five', 'at', 'well', 'nowhere', 'never', 'anyhow', 'does', \"'ve\", 'their', \"'ll\", 'perhaps', 'hereby', 'is', 'should', 'via', 'may', 'down', 'whenever', 'are', '’m', 'afterwards', 'n’t', 'much', 'even', 'sixty', 'again', 'four', 'but', 'please', 'our', 'neither', 'one', 'somehow', 'empty', 'side', 'after', 'last', 'whom', 'latterly', 'any', 'get', 'whereupon', 'thereby', 'just', 'first', 'she', 'besides', 'mine', 'yet', 'without', 'nine', 'give', 'both', 'same', 'moreover', 'since', 'onto', 'be', 'this', 'someone', 'no', 'against', \"'m\", \"'d\", 'meanwhile', 'become', 'sometime', 'whereby', 'everywhere', '’ve', 'them', 'whereas', 'yourself', 'move', '’s', '‘ve', 'others', 'less', 'over', 'than', 'do', 'hers', 'formerly', 'not', 'see', 'cannot', 'him', 'on', 'then', 'most', 'wherein', 'that', '‘d', 'eleven', 'nor', 'hereupon', 'before', 'once', 'too', 'part', 'ours', 'if', 'because', 'ca', 'will', '‘s', 'and', 'when', 'now', 'can', 'towards', 'wherever', 'beforehand', 'up', 'still', 'hundred', 'least', 'rather', 'none', 'among', 'also', 'noone', 'everything', 'namely', 'other', 'in', 'such', 'say', 'us', 'alone', 'what', 'below', 'beside', 'herein', 'off', 'therein', 'was', 'back', 'whither', 'various', 'would', 'myself', 'so', 'latter', 'really', 'her', 'except', 'otherwise', 'put', 'within', 'we', 'could', 'throughout', 'full', 'to', 'through', 'did', 'only', 'there', 'who', 'thereafter', 'with', 'amongst', 'my', 'those', 'former', 'for', 'several', 'third', 'many', 'across', 'some', 'whether', 'therefore', 'very', 'doing', 'anyway', 'enough', '‘m', 'becomes', 'every', 'while', 'were', 'thereupon', 'though', 'beyond', 'by', 'the', 'indeed', 'yourselves', 'go', 'whereafter', 'often', 'using', 'however', 'sometimes', 'am', 'else', 'mostly', 'although', 'it', 'serious', 'became', 'i', '’re', 'they', 'until', '‘re', 'of', 'elsewhere', 'somewhere', 'me', 'during', 'becoming', \"'re\", 'about', 'behind', 'you', 'which', 'a', 'almost', 'have', 'more', 'few', \"'s\", 'fifty', 'next', 'ourselves', 'hereafter', 'eight', 'another', 'along', 'bottom', '’ll', 'seems', 'nobody', 'further', 'n‘t', 'twenty', 'due', 'seem', 'whatever', 'above', 'as', 'your', 'keep', 'each', 'why', 'an', 'top', 'yours', 'six', 'call', 'take', 'together', 'whoever', 'must', 'ever', 'unless', 'something', 'themselves', 'been', 'between', 'into', 'whence', 'being', 'herself', 'itself', 'quite', 'around', 'everyone', 'forty', 'hence', 'whole', 'where', 'anywhere', 'thence', '‘ll', 'regarding', 'used', 'own', 'nothing', 'two', 'twelve', 'per', 'thus', 'has', 'from', 'done', 'made', 'either', 'might', 'ten', 'under', 'upon', 'himself', 'seemed', 'anyone', \"n't\", 'front', 'three', 'already', 'how', 'or', 're', 'had', 'its', 'fifteen', 'seeming', 'show', '’d', 'nevertheless', 'out', 'these', 'name', 'anything', 'always', 'toward', 'amount'}\n"
    }
   ],
   "source": [
    "#停用词\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)#默认停用词\n",
    "#STOP_WORDS.add(u'say')新增停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['hourse', 'gallop', 'field']\n"
    }
   ],
   "source": [
    "#归一化 1、词干提取 2、词根提取\n",
    "doc=nlp(u'the hourse galloped down then field')\n",
    "sentence=[]\n",
    "for w in doc:\n",
    "    if w.text!='n' and not w.is_stop and not w.is_punct:\n",
    "        sentence.append(w.lemma_)#lemma属性获取词根\n",
    "print (sentence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora#导入语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\",\n",
    "u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\",\n",
    "u\"Financial troubles result in loss of millions for bank.\", u\"Western bank files for bankruptcy after financial losses.\", u\"London football club is taken over by oil millionaire from Russia.\", u\"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'], ['weekend', 'football', 'frenzy', 'take', 'London'], ['bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['bank', 'finance', 'work', 'Russia']]\n"
    }
   ],
   "source": [
    "texts=[]\n",
    "for document in documents:\n",
    "    text=[]\n",
    "    doc=nlp(document)\n",
    "    for w in doc:\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)\n",
    "print(texts)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'frenzy': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'takeover': 15, 'Wembley': 16, 'stadium': 17, 'Arsenal': 18, 'Kane': 19, 'pound': 20, 'striker': 21, 'financial': 22, 'loss': 23, 'result': 24, 'trouble': 25, 'bankruptcy': 26, 'file': 27, 'western': 28, 'Russia': 29, 'millionaire': 30, 'oil': 31, 'finance': 32, 'work': 33}\n"
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(texts)#建立词典\n",
    "print(dictionary.token2id)#词dai字典，{词，对应的单词id}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2bow建立词袋\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "print(corpus)\n",
    "#（单词id，单词计数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将语料库存到磁盘：corpora.MmCorpus.serialize('/tmp/example.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 0.4538520228951382), (1, 0.2269260114475691), (2, 0.4538520228951382), (3, 0.16750327793200118), (4, 0.4538520228951382), (5, 0.4538520228951382), (6, 0.3106776504135697)]\n[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.32802654645398593), (8, 0.6560530929079719), (9, 0.4490913847888623)]\n[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n[(1, 0.29431054749542984), (3, 0.21724253258131512), (7, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n[(11, 0.24253562503633297), (18, 0.48507125007266594), (19, 0.48507125007266594), (20, 0.48507125007266594), (21, 0.48507125007266594)]\n[(10, 0.19610384738673725), (13, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.3637247180792822), (24, 0.5313455887718271), (25, 0.5313455887718271)]\n[(10, 0.18286519950508276), (22, 0.3391702611796705), (23, 0.3391702611796705), (26, 0.4954753228542582), (27, 0.4954753228542582), (28, 0.4954753228542582)]\n[(1, 0.2645025265769199), (3, 0.1952400253294319), (7, 0.2645025265769199), (9, 0.3621225392416359), (29, 0.3621225392416359), (30, 0.5290050531538398), (31, 0.5290050531538398)]\n[(10, 0.22867660961662029), (29, 0.4241392327204109), (32, 0.6196018558242014), (33, 0.6196018558242014)]\n"
    }
   ],
   "source": [
    "#使用gensim的语料库训练tf-idf表\n",
    "#TF(词频)一般归一化处理成=词频/文章总词数\n",
    "#IDF（逆向文件频率）=log(总文件数/包含该词的文件数)\n",
    "#tfidf=TF*TDF,tfidf越大，该词的重要性越大\n",
    "from gensim import models\n",
    "tfidf=models.TfidfModel(corpus)\n",
    "for document in tfidf[corpus]:\n",
    "    print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#联系上下文的方法：n-grams \n",
    "bigram=gensim.models.Phrases(texts)#二元，创建bi-gram\n",
    "texts=[bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n ['weekend', 'football', 'frenzy', 'take', 'London'],\n ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'],\n ['Arsenal', 'bid', 'pound', 'striker', 'Kane'],\n ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'],\n ['bank', 'finance', 'work', 'Russia']]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20,no_above=0.5)#删除出现在少于20篇和超过50%的文档中的词"
   ]
  }
 ]
}