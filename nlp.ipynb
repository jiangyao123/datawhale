{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599023675084",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1:分词\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"let's go the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "let's go the park."
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the DET\nhourse NOUN\ngalloped VERB\ndown ADP\nthen ADV\nfield NOUN\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the hourse galloped down then field</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step2:向量化（操作暂忽略）\n",
    "#step3:词性标注\n",
    "for token in doc:\n",
    "    print (token.text,token.pos_)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Europe 0 6 LOC\nShanghai 11 19 GPE\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Europe\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n</mark>\n and \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Shanghai\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step4:语法分析or依存分析（操作暂跳过）\n",
    "# step5:命名实体识别\n",
    "doc=nlp(u'Europe and Shanghai')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_)\n",
    "    #Europe属于地点,上海属于城市  \n",
    "from spacy import displacy#可视化\n",
    "displacy.render(doc,style='ent')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#规则匹配：https://www.jianshu.com/p/488e29470755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'whose', 'here', 'make', 'all', 'he', 'his', 'thru', 'five', 'at', 'well', 'nowhere', 'never', 'anyhow', 'does', \"'ve\", 'their', \"'ll\", 'perhaps', 'hereby', 'is', 'should', 'via', 'may', 'down', 'whenever', 'are', '’m', 'afterwards', 'n’t', 'much', 'even', 'sixty', 'again', 'four', 'but', 'please', 'our', 'neither', 'one', 'somehow', 'empty', 'side', 'after', 'last', 'whom', 'latterly', 'any', 'get', 'whereupon', 'thereby', 'just', 'first', 'she', 'besides', 'mine', 'yet', 'without', 'nine', 'give', 'both', 'same', 'moreover', 'since', 'onto', 'be', 'this', 'someone', 'no', 'against', \"'m\", \"'d\", 'meanwhile', 'become', 'sometime', 'whereby', 'everywhere', '’ve', 'them', 'whereas', 'yourself', 'move', '’s', '‘ve', 'others', 'less', 'over', 'than', 'do', 'hers', 'formerly', 'not', 'see', 'cannot', 'him', 'on', 'then', 'most', 'wherein', 'that', '‘d', 'eleven', 'nor', 'hereupon', 'before', 'once', 'too', 'part', 'ours', 'if', 'because', 'ca', 'will', '‘s', 'and', 'when', 'now', 'can', 'towards', 'wherever', 'beforehand', 'up', 'still', 'hundred', 'least', 'rather', 'none', 'among', 'also', 'noone', 'everything', 'namely', 'other', 'in', 'such', 'say', 'us', 'alone', 'what', 'below', 'beside', 'herein', 'off', 'therein', 'was', 'back', 'whither', 'various', 'would', 'myself', 'so', 'latter', 'really', 'her', 'except', 'otherwise', 'put', 'within', 'we', 'could', 'throughout', 'full', 'to', 'through', 'did', 'only', 'there', 'who', 'thereafter', 'with', 'amongst', 'my', 'those', 'former', 'for', 'several', 'third', 'many', 'across', 'some', 'whether', 'therefore', 'very', 'doing', 'anyway', 'enough', '‘m', 'becomes', 'every', 'while', 'were', 'thereupon', 'though', 'beyond', 'by', 'the', 'indeed', 'yourselves', 'go', 'whereafter', 'often', 'using', 'however', 'sometimes', 'am', 'else', 'mostly', 'although', 'it', 'serious', 'became', 'i', '’re', 'they', 'until', '‘re', 'of', 'elsewhere', 'somewhere', 'me', 'during', 'becoming', \"'re\", 'about', 'behind', 'you', 'which', 'a', 'almost', 'have', 'more', 'few', \"'s\", 'fifty', 'next', 'ourselves', 'hereafter', 'eight', 'another', 'along', 'bottom', '’ll', 'seems', 'nobody', 'further', 'n‘t', 'twenty', 'due', 'seem', 'whatever', 'above', 'as', 'your', 'keep', 'each', 'why', 'an', 'top', 'yours', 'six', 'call', 'take', 'together', 'whoever', 'must', 'ever', 'unless', 'something', 'themselves', 'been', 'between', 'into', 'whence', 'being', 'herself', 'itself', 'quite', 'around', 'everyone', 'forty', 'hence', 'whole', 'where', 'anywhere', 'thence', '‘ll', 'regarding', 'used', 'own', 'nothing', 'two', 'twelve', 'per', 'thus', 'has', 'from', 'done', 'made', 'either', 'might', 'ten', 'under', 'upon', 'himself', 'seemed', 'anyone', \"n't\", 'front', 'three', 'already', 'how', 'or', 're', 'had', 'its', 'fifteen', 'seeming', 'show', '’d', 'nevertheless', 'out', 'these', 'name', 'anything', 'always', 'toward', 'amount'}\n"
    }
   ],
   "source": [
    "#停用词\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)#默认停用词\n",
    "#STOP_WORDS.add(u'say')新增停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['hourse', 'gallop', 'field']\n"
    }
   ],
   "source": [
    "#归一化 1、词干提取 2、词根提取\n",
    "doc=nlp(u'the hourse galloped down then field')\n",
    "sentence=[]\n",
    "for w in doc:\n",
    "    if w.text!='n' and not w.is_stop and not w.is_punct:\n",
    "        sentence.append(w.lemma_)#lemma属性获取词根\n",
    "print (sentence)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora#导入语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\",\n",
    "u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\",\n",
    "u\"Financial troubles result in loss of millions for bank.\", u\"Western bank files for bankruptcy after financial losses.\", u\"London football club is taken over by oil millionaire from Russia.\", u\"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'], ['weekend', 'football', 'frenzy', 'take', 'London'], ['bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['bank', 'finance', 'work', 'Russia']]\n"
    }
   ],
   "source": [
    "texts=[]\n",
    "for document in documents:\n",
    "    text=[]\n",
    "    doc=nlp(document)\n",
    "    for w in doc:\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)\n",
    "print(texts)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'frenzy': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'takeover': 15, 'Wembley': 16, 'stadium': 17, 'Arsenal': 18, 'Kane': 19, 'pound': 20, 'striker': 21, 'financial': 22, 'loss': 23, 'result': 24, 'trouble': 25, 'bankruptcy': 26, 'file': 27, 'western': 28, 'Russia': 29, 'millionaire': 30, 'oil': 31, 'finance': 32, 'work': 33}\n"
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(texts)#建立词典\n",
    "print(dictionary.token2id)#词dai字典，{词，对应的单词id}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2bow建立词袋\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "print(corpus)\n",
    "#（单词id，单词计数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将语料库存到磁盘：corpora.MmCorpus.serialize('/tmp/example.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 0.4538520228951382), (1, 0.2269260114475691), (2, 0.4538520228951382), (3, 0.16750327793200118), (4, 0.4538520228951382), (5, 0.4538520228951382), (6, 0.3106776504135697)]\n[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.32802654645398593), (8, 0.6560530929079719), (9, 0.4490913847888623)]\n[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n[(1, 0.29431054749542984), (3, 0.21724253258131512), (7, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n[(11, 0.24253562503633297), (18, 0.48507125007266594), (19, 0.48507125007266594), (20, 0.48507125007266594), (21, 0.48507125007266594)]\n[(10, 0.19610384738673725), (13, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.3637247180792822), (24, 0.5313455887718271), (25, 0.5313455887718271)]\n[(10, 0.18286519950508276), (22, 0.3391702611796705), (23, 0.3391702611796705), (26, 0.4954753228542582), (27, 0.4954753228542582), (28, 0.4954753228542582)]\n[(1, 0.2645025265769199), (3, 0.1952400253294319), (7, 0.2645025265769199), (9, 0.3621225392416359), (29, 0.3621225392416359), (30, 0.5290050531538398), (31, 0.5290050531538398)]\n[(10, 0.22867660961662029), (29, 0.4241392327204109), (32, 0.6196018558242014), (33, 0.6196018558242014)]\n"
    }
   ],
   "source": [
    "#使用gensim的语料库训练tf-idf表\n",
    "#TF(词频)一般归一化处理成=词频/文章总词数\n",
    "#IDF（逆向文件频率）=log(总文件数/包含该词的文件数)\n",
    "#tfidf=TF*TDF,tfidf越大，该词的重要性越大\n",
    "from gensim import models\n",
    "tfidf=models.TfidfModel(corpus)\n",
    "for document in tfidf[corpus]:\n",
    "    print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#联系上下文的方法：n-grams \n",
    "bigram=gensim.models.Phrases(texts)#二元，创建bi-gram\n",
    "texts=[bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n ['weekend', 'football', 'frenzy', 'take', 'London'],\n ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'],\n ['Arsenal', 'bid', 'pound', 'striker', 'Kane'],\n ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'],\n ['bank', 'finance', 'work', 'Russia']]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20,no_above=0.5)#删除出现在少于20篇和超过50%的文档中的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性标注\n",
    "词性有：\n",
    "名词\n",
    "动词\n",
    "副词\n",
    "带词\n",
    "介词\n",
    "连词\n",
    "感叹词\n",
    "等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/xmly/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import nltk#和spacy库类似\n",
    "#nltk.download('book')首次运行需下载\n",
    "# nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['And', 'now', 'for', 'something', 'completely', 'different']"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\"And now for something completely different\")#tokenize把长句⼦拆成有“意义”的⼩部件\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('And', 'CC'),\n ('now', 'RB'),\n ('for', 'IN'),\n ('something', 'NN'),\n ('completely', 'RB'),\n ('different', 'JJ')]"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "nltk.pos_tag(text)#标注词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC     coordinatingconjunction 并列连词\n",
    "\n",
    "CD     cardinaldigit  纯数  基数\n",
    "\n",
    "DT     determiner  限定词（置于名词前起限定作用，如 the、some、my 等）\n",
    "\n",
    "EX     existentialthere (like:\"there is\"... think of it like \"thereexists\")   存在句；存现句\n",
    "\n",
    "FW     foreignword  外来语；外来词；外文原词\n",
    "\n",
    "IN     preposition/subordinating conjunction介词/从属连词；主从连词；从属连接词\n",
    "\n",
    "JJ     adjective    'big'  形容词\n",
    "\n",
    "JJR    adjective, comparative 'bigger' （形容词或副词的）比较级形式\n",
    "\n",
    "JJS    adjective, superlative 'biggest'  （形容词或副词的）最高级\n",
    "\n",
    "LS     listmarker  1)\n",
    "\n",
    "MD     modal (could, will) 形态的，形式的 , 语气的；情态的\n",
    "\n",
    "NN     noun, singular 'desk' 名词单数形式\n",
    "\n",
    "NNS    nounplural  'desks'  名词复数形式\n",
    "\n",
    "NNP    propernoun, singular     'Harrison' 专有名词\n",
    "\n",
    "NNPS  proper noun, plural 'Americans'  专有名词复数形式\n",
    "\n",
    "PDT    predeterminer      'all the kids'  前位限定词\n",
    "\n",
    "POS    possessiveending  parent's   属有词  结束语\n",
    "\n",
    "PRP    personalpronoun   I, he, she  人称代词\n",
    "\n",
    "PRP$  possessive pronoun my, his, hers  物主代词\n",
    "\n",
    "RB     adverb very, silently, 副词    非常  静静地\n",
    "\n",
    "RBR    adverb,comparative better   （形容词或副词的）比较级形式\n",
    "\n",
    "RBS    adverb,superlative best    （形容词或副词的）最高级\n",
    "\n",
    "RP     particle     give up 小品词(与动词构成短语动词的副词或介词)\n",
    "\n",
    "TO     to    go 'to' the store.\n",
    "\n",
    "UH     interjection errrrrrrrm  感叹词；感叹语\n",
    "\n",
    "VB     verb, baseform    take   动词\n",
    "\n",
    "VBD    verb, pasttense   took   动词   过去时；过去式\n",
    "\n",
    "VBG    verb,gerund/present participle taking 动词  动名词/现在分词\n",
    "\n",
    "VBN    verb, pastparticiple     taken 动词  过去分词\n",
    "\n",
    "VBP    verb,sing. present, non-3d     take 动词  现在\n",
    "\n",
    "VBZ    verb, 3rdperson sing. present  takes   动词  第三人称\n",
    "\n",
    "WDT    wh-determiner      which 限定词（置于名词前起限定作用，如 the、some、my 等）\n",
    "\n",
    "WP     wh-pronoun   who, what 代词（代替名词或名词词组的单词）\n",
    "\n",
    "WP$    possessivewh-pronoun     whose  所有格；属有词\n",
    "\n",
    "WRB    wh-abverb    where, when 副词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Mathieu and I went to the park.')\n",
    "sent_1 = nlp(u'If Clement was asked to take out the garbage, he would refuse.')\n",
    "sent_2 = nlp(u'Baptiste was in charge of the refuse treatment center.')\n",
    "sent_3 = nlp(u'Marie took out her rather suspicious and fishy cat to fish for fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mathieu PROPN NNP\nand CCONJ CC\nI PRON PRP\nwent VERB VBD\nto ADP IN\nthe DET DT\npark NOUN NN\n. PUNCT .\n"
    }
   ],
   "source": [
    "for token in sent_0:\n",
    "    print(token.text,token.pos_,token.tag_)#pos粗粒度词性；tag细粒度词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Marie PROPN NNP\ntook VERB VBD\nout ADP RP\nher PRON PRP\nrather ADV RB\nsuspicious ADJ JJ\nand CCONJ CC\nfishy ADJ JJ\ncat NOUN NN\nto PART TO\nfish VERB VB\nfor ADP IN\nfish NOUN NN\n. PUNCT .\n"
    }
   ],
   "source": [
    "for token in sent_3:\n",
    "    print(token.text,token.pos_,token.tag_)#spacy可通过上下文标注词性（两个fish的不同标注）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义标注器(没太明白，暂缓)\n",
    "TRAIN_DATA = [\n",
    "     (\"Facebook has been accused for leaking personal data of users.\",\n",
    "{'entities': [(0, 8, 'ORG')]}),\n",
    "      (\"Tinder uses sophisticated algorithms to find the perfect match.\",\n",
    "{'entities': [(0, 6, \"ORG\")]})]\n",
    "\n",
    "nlp = spacy.blank('en')#创建空模型\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk('/model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER标注（命名实体识别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree,tree2conlltags   \n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk import ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则：https://blog.csdn.net/qq_40407889/article/details/101055602\n",
    "\n",
    "https://www.runoob.com/python/python-reg-expressions.html#flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex_str = [\n",
    "emoticons_str,\n",
    "r'<[^>]+>', # HTML tags\n",
    "'''重复修饰符 (*, +, ?, {m,n}, 等) 不能直接嵌套。这样避免了非贪婪后缀 ? 修饰符，和其他实现中的修饰符产生的多义性。要应用一个内层重复嵌套，可以使用括号。 比如，表达式 (?:a{6})* 匹配6个 'a' 字符重复任意次数。'''\n",
    "r'(?:@[\\w_]+)', # @某⼈人\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "# URLs\n",
    "r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词\n",
    "r'(?:[\\w_]+)', # 其他\n",
    "r'(?:\\S)' # 其他\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'industry'"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "'''(?:pattern) \n",
    "非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。'''\n",
    "import re\n",
    "doc=re.match('industr(?:y|ies)','industry')\n",
    "doc.group(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'industr'"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "'''(?=pattern)\n",
    "非获取匹配，正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。'''\n",
    "doc=re.match('industr(?=y|ies)','industry')\n",
    "doc.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'industr'"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "'''(?!pattern)\n",
    "非获取匹配，正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。'''\n",
    "doc=re.search('industr(?!y|ies)','industr')\n",
    "doc.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Windows'"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "'''(?<=pattern)\n",
    "非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?<=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。'''\n",
    "doc=re.search('(?<=NT)Windows','NTWindows')#测试了下，不支持(?<=95|98|NT|2000)格式，原因未知\n",
    "doc.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Windows'"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "'''反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?<!95|98|NT|2000)Windows“能匹配”3.1Windows“中的”Windows\"，但不能匹配\"2000Windows“中的”Windows\"。'''\n",
    "doc=re.search('(?<!95)Windows','3.1Windows')#同上，无法匹配多个\n",
    "doc.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['123', '456']\n['88', '12']\n"
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d+',re.I)   # 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象,re.I表示匹配规则，这里指不区分大小写\n",
    "result1 = pattern.findall('runoob 123 google 456')#findall匹配全部，search 和 match 仅匹配一次\n",
    "result2 = pattern.findall('run88oob123google456', 0, 10)#指定匹配字符位置从0开始，长度为10\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bat', 'bit', 'hat']\n"
    }
   ],
   "source": [
    "#1. 写一个正则表达式，使其能同时识别下面所有的字符串：'bat','bit', 'but', 'hat', 'hit', 'hut'\n",
    "pattern = re.compile(r'[bh][ait]t')\n",
    "result=pattern.findall('baterbitefrehat')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('mike', 'li'), ('li', 'lei')]\n"
    }
   ],
   "source": [
    "# 2.匹配由单个空格分隔的任意单词对，也就是姓和名\n",
    "pattern = re.compile(r'([a-zA-Z]+) ([a-zA-Z]+)')#中间手动写空格\n",
    "result=pattern.findall('mike li, li lei')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "#mao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "#锚字符：标记模式在数据流的中开始或结尾的位置：^和$\n",
    "result=re.search('^test', 'This is a\\ntest of a new line')#^在每个字符串的开始处查找\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(10, 14), match='test'>\n"
    }
   ],
   "source": [
    "result=re.search('^test', 'This is a\\ntest of a new line',re.M)#^要每行开头匹配的话，加上re.M\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(15, 19), match='book'>\nNone\n"
    }
   ],
   "source": [
    "result1=re.search('book$', 'This is a good book')#$查找结尾\n",
    "result2=re.search('book$', 'This book is good')\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 14), match='this is a test'>\nNone\n<re.Match object; span=(0, 0), match=''>\n"
    }
   ],
   "source": [
    "#组合锚\n",
    "result1=re.search('^this is a test$', 'this is a test')\n",
    "result2=re.search('^$', 'This is a test string')\n",
    "result3=re.search('^$', '')#过滤空字符\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(4, 7), match='cat'>\n<re.Match object; span=(1, 4), match='hat'>\n<re.Match object; span=(5, 8), match=' at'>\nNone\n"
    }
   ],
   "source": [
    "# 点字符，匹配除换行符以外任何一个字符，但点字符前必须得有字符，否则匹配失败\n",
    "result1=re.search('.at', 'The cat is sleeping')\n",
    "result2=re.search('.at', 'That is heavy')\n",
    "result3=re.search('.at', 'He is at the store')\n",
    "result4=re.search('.at', 'at the top of the hour')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 5), match='12345'>\nNone\nNone\n"
    }
   ],
   "source": [
    "#字符分类\n",
    "result1=re.search('^[0123456789][0123456789][0123456789][0123456789][0123456789]$', '12345')#匹配5位数字\n",
    "result2=re.search('^[0123456789][0123456789][0123456789][0123456789][0123456789]$', '123456')\n",
    "result3=re.search('^[0123456789][0123456789][0123456789][0123456789][0123456789]$','1234')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n<re.Match object; span=(5, 8), match=' at'>\nNone\n"
    }
   ],
   "source": [
    "#字符分类取反\n",
    "result1=re.search('[^ch]at', 'The cat is sleeping')\n",
    "result2=re.search('[^ch]at', 'He is at home')\n",
    "result3=re.search('[^ch]at', 'at the top of the hour')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<re.Match object; span=(0, 5), match='12345'>"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "#使用范围\n",
    "re.search('^[0-9][0-9][0-9][0-9][0-9]$', '12345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 2), match='ik'>\n<re.Match object; span=(0, 3), match='iek'>\n<re.Match object; span=(0, 5), match='ieeek'>\n"
    }
   ],
   "source": [
    "#星号：在一个字符后面放一个星号，表示该字符可以出现0次或多次\n",
    "result1=re.search('ie*k', 'ik')\n",
    "result2=re.search('ie*k', 'iek')\n",
    "result3=re.search('ie*k', 'ieeek')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(10, 36), match='regular pattern expression'>\n"
    }
   ],
   "source": [
    "result=re.search('regular.*expression', 'This is a regular pattern expression')#匹配任意数量的任意字符\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 2), match='bt'>\n<re.Match object; span=(0, 3), match='bet'>\nNone\n"
    }
   ],
   "source": [
    "#问号表示前面的字符出现0或1次\n",
    "result1=re.search('be?t', 'bt')\n",
    "result2=re.search('be?t', 'bet')\n",
    "result3=re.search('be?t', 'beet')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n<re.Match object; span=(0, 4), match='beet'>\n"
    }
   ],
   "source": [
    "#加号表示前面的字符出现1或多次\n",
    "result1=re.search('be+t', 'bt')\n",
    "result2=re.search('be+t', 'beet')\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n<re.Match object; span=(0, 4), match='beet'>\n"
    }
   ],
   "source": [
    "#使用大括号,表示出现次数的区间\n",
    "result1=re.search('be{1,2}t', 'bt')\n",
    "result2=re.search('be{1,2}t', 'beet')\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(4, 7), match='cat'>\n<re.Match object; span=(4, 7), match='dog'>\nNone\n"
    }
   ],
   "source": [
    "#管道符号，表示任意一个模式匹配 expr1 | expr2 |...\n",
    "result1=re.search('cat|dog', 'The cat is sleeping')\n",
    "result2=re.search('cat|dog', 'The dog is sleeping')\n",
    "result3=re.search('cat|dog', 'The horse is sleeping')\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 3), match='Sat'>\n<re.Match object; span=(0, 8), match='Saturday'>\n<re.Match object; span=(0, 3), match='cab'>\n"
    }
   ],
   "source": [
    "#分组表达式，使用括号分组，这个组会当成标准字符来对待\n",
    "result1=re.search('Sat(urday)?', 'Sat')\n",
    "result2=re.search('Sat(urday)?', 'Saturday')\n",
    "result3=re.search('(c|b)a(b|t)', 'cab')#匹配第一个分组中字母和第二个分组中字母的任意组合\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<re.Match object; span=(0, 12), match='123-456-7890'>\n"
    }
   ],
   "source": [
    "'''匹配手机号,四种格式\n",
    "(123)456-7890\n",
    "(123) 456-7890\n",
    "123-456-7890\n",
    "123.456.7890\n",
    "'''\n",
    "pattern = re.compile(r'^\\(?[1-9][0-9]{2}\\)?(| |-|\\.)[0-9]{3}( |-|\\.)[0-9]{4}$')\n",
    "result1 = pattern.search('123-456-7890')\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}