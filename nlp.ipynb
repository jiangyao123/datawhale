{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599023675084",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1:分词\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"let's go the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "let's go the park."
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the DET\nhourse NOUN\ngalloped VERB\ndown ADP\nthen ADV\nfield NOUN\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the hourse galloped down then field</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step2:向量化（操作暂忽略）\n",
    "#step3:词性标注\n",
    "for token in doc:\n",
    "    print (token.text,token.pos_)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Europe 0 6 LOC\nShanghai 11 19 GPE\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Europe\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n</mark>\n and \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Shanghai\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#step4:语法分析or依存分析（操作暂跳过）\n",
    "# step5:命名实体识别\n",
    "doc=nlp(u'Europe and Shanghai')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.start_char,ent.end_char,ent.label_)\n",
    "    #Europe属于地点,上海属于城市  \n",
    "from spacy import displacy#可视化\n",
    "displacy.render(doc,style='ent')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#规则匹配：https://www.jianshu.com/p/488e29470755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'whose', 'here', 'make', 'all', 'he', 'his', 'thru', 'five', 'at', 'well', 'nowhere', 'never', 'anyhow', 'does', \"'ve\", 'their', \"'ll\", 'perhaps', 'hereby', 'is', 'should', 'via', 'may', 'down', 'whenever', 'are', '’m', 'afterwards', 'n’t', 'much', 'even', 'sixty', 'again', 'four', 'but', 'please', 'our', 'neither', 'one', 'somehow', 'empty', 'side', 'after', 'last', 'whom', 'latterly', 'any', 'get', 'whereupon', 'thereby', 'just', 'first', 'she', 'besides', 'mine', 'yet', 'without', 'nine', 'give', 'both', 'same', 'moreover', 'since', 'onto', 'be', 'this', 'someone', 'no', 'against', \"'m\", \"'d\", 'meanwhile', 'become', 'sometime', 'whereby', 'everywhere', '’ve', 'them', 'whereas', 'yourself', 'move', '’s', '‘ve', 'others', 'less', 'over', 'than', 'do', 'hers', 'formerly', 'not', 'see', 'cannot', 'him', 'on', 'then', 'most', 'wherein', 'that', '‘d', 'eleven', 'nor', 'hereupon', 'before', 'once', 'too', 'part', 'ours', 'if', 'because', 'ca', 'will', '‘s', 'and', 'when', 'now', 'can', 'towards', 'wherever', 'beforehand', 'up', 'still', 'hundred', 'least', 'rather', 'none', 'among', 'also', 'noone', 'everything', 'namely', 'other', 'in', 'such', 'say', 'us', 'alone', 'what', 'below', 'beside', 'herein', 'off', 'therein', 'was', 'back', 'whither', 'various', 'would', 'myself', 'so', 'latter', 'really', 'her', 'except', 'otherwise', 'put', 'within', 'we', 'could', 'throughout', 'full', 'to', 'through', 'did', 'only', 'there', 'who', 'thereafter', 'with', 'amongst', 'my', 'those', 'former', 'for', 'several', 'third', 'many', 'across', 'some', 'whether', 'therefore', 'very', 'doing', 'anyway', 'enough', '‘m', 'becomes', 'every', 'while', 'were', 'thereupon', 'though', 'beyond', 'by', 'the', 'indeed', 'yourselves', 'go', 'whereafter', 'often', 'using', 'however', 'sometimes', 'am', 'else', 'mostly', 'although', 'it', 'serious', 'became', 'i', '’re', 'they', 'until', '‘re', 'of', 'elsewhere', 'somewhere', 'me', 'during', 'becoming', \"'re\", 'about', 'behind', 'you', 'which', 'a', 'almost', 'have', 'more', 'few', \"'s\", 'fifty', 'next', 'ourselves', 'hereafter', 'eight', 'another', 'along', 'bottom', '’ll', 'seems', 'nobody', 'further', 'n‘t', 'twenty', 'due', 'seem', 'whatever', 'above', 'as', 'your', 'keep', 'each', 'why', 'an', 'top', 'yours', 'six', 'call', 'take', 'together', 'whoever', 'must', 'ever', 'unless', 'something', 'themselves', 'been', 'between', 'into', 'whence', 'being', 'herself', 'itself', 'quite', 'around', 'everyone', 'forty', 'hence', 'whole', 'where', 'anywhere', 'thence', '‘ll', 'regarding', 'used', 'own', 'nothing', 'two', 'twelve', 'per', 'thus', 'has', 'from', 'done', 'made', 'either', 'might', 'ten', 'under', 'upon', 'himself', 'seemed', 'anyone', \"n't\", 'front', 'three', 'already', 'how', 'or', 're', 'had', 'its', 'fifteen', 'seeming', 'show', '’d', 'nevertheless', 'out', 'these', 'name', 'anything', 'always', 'toward', 'amount'}\n"
    }
   ],
   "source": [
    "#停用词\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)#默认停用词\n",
    "#STOP_WORDS.add(u'say')新增停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['hourse', 'gallop', 'field']\n"
    }
   ],
   "source": [
    "#归一化 1、词干提取 2、词根提取\n",
    "doc=nlp(u'the hourse galloped down then field')\n",
    "sentence=[]\n",
    "for w in doc:\n",
    "    if w.text!='n' and not w.is_stop and not w.is_punct:\n",
    "        sentence.append(w.lemma_)#lemma属性获取词根\n",
    "print (sentence)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora#导入语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [u\"Football club Arsenal defeat local rivals this weekend.\",\n",
    "u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\",\n",
    "u\"Financial troubles result in loss of millions for bank.\", u\"Western bank files for bankruptcy after financial losses.\", u\"London football club is taken over by oil millionaire from Russia.\", u\"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'], ['weekend', 'football', 'frenzy', 'take', 'London'], ['bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['bank', 'finance', 'work', 'Russia']]\n"
    }
   ],
   "source": [
    "texts=[]\n",
    "for document in documents:\n",
    "    text=[]\n",
    "    doc=nlp(document)\n",
    "    for w in doc:\n",
    "        if not w.is_stop and not w.is_punct and not w.like_num:\n",
    "            text.append(w.lemma_)\n",
    "    texts.append(text)\n",
    "print(texts)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'frenzy': 8, 'take': 9, 'bank': 10, 'bid': 11, 'lose': 12, 'million': 13, 'open': 14, 'takeover': 15, 'Wembley': 16, 'stadium': 17, 'Arsenal': 18, 'Kane': 19, 'pound': 20, 'striker': 21, 'financial': 22, 'loss': 23, 'result': 24, 'trouble': 25, 'bankruptcy': 26, 'file': 27, 'western': 28, 'Russia': 29, 'millionaire': 30, 'oil': 31, 'finance': 32, 'work': 33}\n"
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(texts)#建立词典\n",
    "print(dictionary.token2id)#词dai字典，{词，对应的单词id}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2bow建立词袋\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "print(corpus)\n",
    "#（单词id，单词计数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将语料库存到磁盘：corpora.MmCorpus.serialize('/tmp/example.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 0.4538520228951382), (1, 0.2269260114475691), (2, 0.4538520228951382), (3, 0.16750327793200118), (4, 0.4538520228951382), (5, 0.4538520228951382), (6, 0.3106776504135697)]\n[(3, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.32802654645398593), (8, 0.6560530929079719), (9, 0.4490913847888623)]\n[(10, 0.18797844084016113), (11, 0.25466485399352906), (12, 0.5093297079870581), (13, 0.3486540744136096), (14, 0.5093297079870581), (15, 0.5093297079870581)]\n[(1, 0.29431054749542984), (3, 0.21724253258131512), (7, 0.29431054749542984), (11, 0.29431054749542984), (16, 0.5886210949908597), (17, 0.5886210949908597)]\n[(11, 0.24253562503633297), (18, 0.48507125007266594), (19, 0.48507125007266594), (20, 0.48507125007266594), (21, 0.48507125007266594)]\n[(10, 0.19610384738673725), (13, 0.3637247180792822), (22, 0.3637247180792822), (23, 0.3637247180792822), (24, 0.5313455887718271), (25, 0.5313455887718271)]\n[(10, 0.18286519950508276), (22, 0.3391702611796705), (23, 0.3391702611796705), (26, 0.4954753228542582), (27, 0.4954753228542582), (28, 0.4954753228542582)]\n[(1, 0.2645025265769199), (3, 0.1952400253294319), (7, 0.2645025265769199), (9, 0.3621225392416359), (29, 0.3621225392416359), (30, 0.5290050531538398), (31, 0.5290050531538398)]\n[(10, 0.22867660961662029), (29, 0.4241392327204109), (32, 0.6196018558242014), (33, 0.6196018558242014)]\n"
    }
   ],
   "source": [
    "#使用gensim的语料库训练tf-idf表\n",
    "#TF(词频)一般归一化处理成=词频/文章总词数\n",
    "#IDF（逆向文件频率）=log(总文件数/包含该词的文件数)\n",
    "#tfidf=TF*TDF,tfidf越大，该词的重要性越大\n",
    "from gensim import models\n",
    "tfidf=models.TfidfModel(corpus)\n",
    "for document in tfidf[corpus]:\n",
    "    print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#联系上下文的方法：n-grams \n",
    "bigram=gensim.models.Phrases(texts)#二元，创建bi-gram\n",
    "texts=[bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['football', 'club', 'arsenal', 'defeat', 'local', 'rival', 'weekend'],\n ['weekend', 'football', 'frenzy', 'take', 'London'],\n ['bank', 'open', 'takeover', 'bid', 'lose', 'million'],\n ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'],\n ['Arsenal', 'bid', 'pound', 'striker', 'Kane'],\n ['financial', 'trouble', 'result', 'loss', 'million', 'bank'],\n ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'],\n ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'],\n ['bank', 'finance', 'work', 'Russia']]"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(1, 1), (3, 1), (7, 1), (11, 1), (16, 1), (17, 1)], [(11, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(10, 1), (13, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(10, 1), (22, 1), (23, 1), (26, 1), (27, 1), (28, 1)], [(1, 1), (3, 1), (7, 1), (9, 1), (29, 1), (30, 1), (31, 1)], [(10, 1), (29, 1), (32, 1), (33, 1)]]\n"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20,no_above=0.5)#删除出现在少于20篇和超过50%的文档中的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性标注\n",
    "词性有：\n",
    "名词\n",
    "动词\n",
    "副词\n",
    "带词\n",
    "介词\n",
    "连词\n",
    "感叹词\n",
    "等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/xmly/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import nltk#和spacy库类似\n",
    "#nltk.download('book')首次运行需下载\n",
    "# nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['And', 'now', 'for', 'something', 'completely', 'different']"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\"And now for something completely different\")#tokenize把长句⼦拆成有“意义”的⼩部件\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('And', 'CC'),\n ('now', 'RB'),\n ('for', 'IN'),\n ('something', 'NN'),\n ('completely', 'RB'),\n ('different', 'JJ')]"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "nltk.pos_tag(text)#标注词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC     coordinatingconjunction 并列连词\n",
    "\n",
    "CD     cardinaldigit  纯数  基数\n",
    "\n",
    "DT     determiner  限定词（置于名词前起限定作用，如 the、some、my 等）\n",
    "\n",
    "EX     existentialthere (like:\"there is\"... think of it like \"thereexists\")   存在句；存现句\n",
    "\n",
    "FW     foreignword  外来语；外来词；外文原词\n",
    "\n",
    "IN     preposition/subordinating conjunction介词/从属连词；主从连词；从属连接词\n",
    "\n",
    "JJ     adjective    'big'  形容词\n",
    "\n",
    "JJR    adjective, comparative 'bigger' （形容词或副词的）比较级形式\n",
    "\n",
    "JJS    adjective, superlative 'biggest'  （形容词或副词的）最高级\n",
    "\n",
    "LS     listmarker  1)\n",
    "\n",
    "MD     modal (could, will) 形态的，形式的 , 语气的；情态的\n",
    "\n",
    "NN     noun, singular 'desk' 名词单数形式\n",
    "\n",
    "NNS    nounplural  'desks'  名词复数形式\n",
    "\n",
    "NNP    propernoun, singular     'Harrison' 专有名词\n",
    "\n",
    "NNPS  proper noun, plural 'Americans'  专有名词复数形式\n",
    "\n",
    "PDT    predeterminer      'all the kids'  前位限定词\n",
    "\n",
    "POS    possessiveending  parent's   属有词  结束语\n",
    "\n",
    "PRP    personalpronoun   I, he, she  人称代词\n",
    "\n",
    "PRP$  possessive pronoun my, his, hers  物主代词\n",
    "\n",
    "RB     adverb very, silently, 副词    非常  静静地\n",
    "\n",
    "RBR    adverb,comparative better   （形容词或副词的）比较级形式\n",
    "\n",
    "RBS    adverb,superlative best    （形容词或副词的）最高级\n",
    "\n",
    "RP     particle     give up 小品词(与动词构成短语动词的副词或介词)\n",
    "\n",
    "TO     to    go 'to' the store.\n",
    "\n",
    "UH     interjection errrrrrrrm  感叹词；感叹语\n",
    "\n",
    "VB     verb, baseform    take   动词\n",
    "\n",
    "VBD    verb, pasttense   took   动词   过去时；过去式\n",
    "\n",
    "VBG    verb,gerund/present participle taking 动词  动名词/现在分词\n",
    "\n",
    "VBN    verb, pastparticiple     taken 动词  过去分词\n",
    "\n",
    "VBP    verb,sing. present, non-3d     take 动词  现在\n",
    "\n",
    "VBZ    verb, 3rdperson sing. present  takes   动词  第三人称\n",
    "\n",
    "WDT    wh-determiner      which 限定词（置于名词前起限定作用，如 the、some、my 等）\n",
    "\n",
    "WP     wh-pronoun   who, what 代词（代替名词或名词词组的单词）\n",
    "\n",
    "WP$    possessivewh-pronoun     whose  所有格；属有词\n",
    "\n",
    "WRB    wh-abverb    where, when 副词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Mathieu and I went to the park.')\n",
    "sent_1 = nlp(u'If Clement was asked to take out the garbage, he would refuse.')\n",
    "sent_2 = nlp(u'Baptiste was in charge of the refuse treatment center.')\n",
    "sent_3 = nlp(u'Marie took out her rather suspicious and fishy cat to fish for fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mathieu PROPN NNP\nand CCONJ CC\nI PRON PRP\nwent VERB VBD\nto ADP IN\nthe DET DT\npark NOUN NN\n. PUNCT .\n"
    }
   ],
   "source": [
    "for token in sent_0:\n",
    "    print(token.text,token.pos_,token.tag_)#pos粗粒度词性；tag细粒度词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Marie PROPN NNP\ntook VERB VBD\nout ADP RP\nher PRON PRP\nrather ADV RB\nsuspicious ADJ JJ\nand CCONJ CC\nfishy ADJ JJ\ncat NOUN NN\nto PART TO\nfish VERB VB\nfor ADP IN\nfish NOUN NN\n. PUNCT .\n"
    }
   ],
   "source": [
    "for token in sent_3:\n",
    "    print(token.text,token.pos_,token.tag_)#spacy可通过上下文标注词性（两个fish的不同标注）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义标注器(没太明白，暂缓)\n",
    "TRAIN_DATA = [\n",
    "     (\"Facebook has been accused for leaking personal data of users.\",\n",
    "{'entities': [(0, 8, 'ORG')]}),\n",
    "      (\"Tinder uses sophisticated algorithms to find the perfect match.\",\n",
    "{'entities': [(0, 6, \"ORG\")]})]\n",
    "\n",
    "nlp = spacy.blank('en')#创建空模型\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk('/model')\n",
    "\n",
    "\n"
   ]
  }
 ]
}